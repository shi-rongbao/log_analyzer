# Web 服务器日志分析器

这是一个用于分析 Web 服务器访问日志的命令行脚本。该脚本能够从一个 JSON 格式的日志文件中提取关键的性能和使用指标。

## 如何运行 (How to Run)

### 环境要求

- Python 3.6 或更高版本
- 无需安装任何第三方库

### 执行指令

1.  将 `log_analyzer.py` 和 `access.log` 文件放置在同一个目录下。
2.  打开你的终端或命令行工具。
3.  执行以下命令：

    ```bash
    python log_analyzer.py access.log
    ```

### 预期输出

脚本会读取 `access.log` 文件，进行分析，然后在终端打印出如下格式的 JSON 结果：

```json
{
  "total_requests": 10,
  "average_response_time_ms": 208.5,
  "status_code_counts": {
    "200": 6,
    "404": 2,
    "500": 2
  },
  "busiest_hour": 11
}
```

---

## 实现思路 (Implementation Approach)

我的实现思路侧重于效率、可读性和健壮性。

1.  **文件处理**:
    - 我选择**逐行读取**文件，而不是一次性将整个文件加载到内存中。这是一个关键决策，因为它确保了即使在处理非常大的日志文件时，脚本的内存占用也是极低的（$O(1)$ 空间复杂度，不考虑存储统计数据本身）。

2.  **数据结构**:
    - **总请求数 (`total_requests`) 和总响应时间 (`total_response_time_sum`)**: 使用简单的整数/浮点数累加器，这是最直接和高效的方法。
    - **HTTP状态码统计 (`status_code_counts`)**: 我使用了一个**哈希表（在 Python 中是字典 `dict`）**。
        - **选择原因**: 哈希表为每种状态码提供了一个键值对存储。当处理到一行日志时，我们可以用状态码作为键，在平均 $O(1)$ 的时间复杂度内完成查找和更新其计数值。这比其他结构（如列表）要快得多。
    - **小时请求统计 (`hourly_requests`)**: 同样使用了**哈希表**，键是小时（0-23），值是该小时内的请求数量。这使得统计一天中最繁忙小时的操作变得非常高效。

3.  **时间处理**:
    - 日志中的时间戳是 ISO 8601 格式 (`YYYY-MM-DDTHH:MM:SSZ`)。我直接截取小时部分 (`HH`) 进行统计，因为对于这个问题，我们不需要更精细的日期或分钟信息。这种字符串处理方式比完整的日期时间解析要快一些，但如果未来需要更复杂的时间分析（如按天、按周），则应使用标准的日期时间库进行解析。

4.  **代码结构**:
    - 我将核心逻辑封装在 `analyze_log` 函数中，使得代码模块化，易于测试和维护。
    - 使用 `if __name__ == "__main__":` 结构，这是 Python 的最佳实践，确保代码在被作为脚本执行时运行，而在被其他模块导入时不会自动执行。
    - 使用 `argparse` 模块来处理命令行参数，这比手动解析 `sys.argv` 更健壮、更用户友好。

---

## 可以优化的地方 (Potential Optimizations)

当前实现对于中小型文件（几百MB到1GB）表现良好，但如果日志文件变得非常大（例如 10GB 或更大），会遇到以下问题和相应的改进方案：

**问题：处理速度瓶颈**

虽然内存使用是高效的，但单线程处理一个 10GB 的文件会非常耗时。整个过程受限于单个 CPU 核心的处理速度和磁盘 I/O 速度。

**改进方案：并行处理 (Parallel Processing)**

我们可以利用多核 CPU 来显著加快处理速度。

1.  **分块处理**:
    - 将大文件逻辑上或物理上分割成多个块（chunks）。
    - 启动一个进程池（例如，使用 Python 的 `multiprocessing` 模块），每个进程独立分析一个文件块。

2.  **MapReduce 思想**:
    - **Map 阶段**: 每个子进程计算其负责块的统计数据（例如，该块的总请求数、响应时间和各种计数器）。
    - **Reduce 阶段**: 主进程收集所有子进程返回的局部统计结果，并将它们合并成最终的全局统计结果。
        - 合并 `total_requests`：简单相加。
        - 合并 `average_response_time`：将所有块的 `total_response_time` 相加，再除以所有块的 `total_requests` 之和。
        - 合并 `status_code_counts` 和 `hourly_requests`：遍历每个子进程返回的计数字典，将它们的计数累加到主计数字典中。

这种并行化方法可以将处理时间大致缩短为 `原时间 / CPU核心数`，从而有效应对超大文件的挑战。对于TB级别的数据，则需要考虑使用像 Apache Spark 这样的分布式计算框架。
